{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f5f217",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd62042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fd62042",
    "outputId": "8c854784-a05c-443e-a99d-3afe95202436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f57c02cb1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58f506",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d25ad",
   "metadata": {
    "id": "248d25ad"
   },
   "outputs": [],
   "source": [
    "DROPOUT = 0.2\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYER = 4\n",
    "BLOCK_SIZE = 64\n",
    "NUM_EMBED = 128\n",
    "BATCH_SIZE = 64\n",
    "EVAL_ITERS = 200\n",
    "EVAL_INTERVAL = 500\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_ITERATION = 10000\n",
    "MAX_NEW_TOKENS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116f934",
   "metadata": {},
   "source": [
    "### Get the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6722f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c6722f7",
    "outputId": "94afdf6e-eb12-4dae-98a9-177f10557005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = get_file(\"shakespeare.txt\",\n",
    "                        \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "\n",
    "text = open(path_to_file, \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e415a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "262e415a",
    "outputId": "4fa72167-682a-48a0-e73d-a161a21effaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbc2e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9fbc2e9",
    "outputId": "4c080bba-9811-40ab-d792-93b2e9ef909e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "#print(f\"Length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab191286",
   "metadata": {},
   "source": [
    "### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2c671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3a2c671",
    "outputId": "ab9c4749-2dd3-46e3-85d5-302ddff781df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "VOCABULARY SIZE: 65\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(characters)\n",
    "#print(f\"\"\"VOCABULARY: {\"\".join(characters)}\n",
    "#VOCABULARY SIZE: {VOCAB_SIZE}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5063c",
   "metadata": {
    "id": "efa5063c"
   },
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(characters)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e74ddc",
   "metadata": {},
   "source": [
    "### Label encoder  \n",
    "#### text-to-numeric vector conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a1f2b",
   "metadata": {
    "id": "8f2a1f2b"
   },
   "outputs": [],
   "source": [
    "encode = lambda string: [string_to_int[char] for char in string]\n",
    "decode = lambda integer: \"\".join([int_to_string[num] for num in integer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88969265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88969265",
    "outputId": "f9a1da00-2290-450e-a52d-536025242064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 21, 1, 39, 51, 1, 31, 53, 59, 51, 53, 42, 43, 43, 54] \n",
      " Hello I am Soumodeep\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello I am Soumodeep\"\n",
    "integer = encode(string)\n",
    "\n",
    "#print(integer,\"\\n\",decode(integer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b025241",
   "metadata": {},
   "source": [
    "### Store data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485135dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "485135dc",
    "outputId": "a8f008b1-6671-412e-c03c-fb0e20dfa61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data= torch.tensor(encode(text), dtype=torch.long)\n",
    "#print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f3f53",
   "metadata": {},
   "source": [
    "### Training and Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7f903",
   "metadata": {
    "id": "0fb7f903"
   },
   "outputs": [],
   "source": [
    "n = int(0.9*data.shape[0])\n",
    "training_data = data[:n]\n",
    "validation_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35ad48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f35ad48",
    "outputId": "20e92e90-14ec-4117-fe4f-8d403016d3a7"
   },
   "outputs": [],
   "source": [
    "#x = training_data[:BLOCK_SIZE+1]\n",
    "#\n",
    "#for i in range(BLOCK_SIZE):\n",
    "#    context = x[:i+1]\n",
    "#    target = x[i+1]\n",
    "#    print(f\"Input {context} => Output {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918b7de",
   "metadata": {
    "id": "9918b7de"
   },
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    \"\"\"\n",
    "    This function takes a corpus and returns a batch of input and \n",
    "    output strings where outputs are one block shifted to the right.\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = torch.randint(len(data)-BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    \n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in idx])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1f0a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52e1f0a3",
    "outputId": "e56f69bc-2f68-45c5-be57-deb5aadc28ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor([[50, 43,  8,  ..., 53, 51, 43],\n",
      "        [ 1, 58, 46,  ..., 33, 25, 14],\n",
      "        [42,  1, 61,  ..., 51,  1, 52],\n",
      "        ...,\n",
      "        [58, 56, 43,  ..., 13, 52, 53],\n",
      "        [44, 50, 47,  ..., 56, 43, 39],\n",
      "        [43,  8,  0,  ...,  0, 25, 63]])\n",
      "Input shape torch.Size([64, 64])\n",
      "Output tensor([[43,  8,  1,  ..., 51, 43, 52],\n",
      "        [58, 46, 43,  ..., 25, 14, 17],\n",
      "        [ 1, 61, 47,  ...,  1, 52, 53],\n",
      "        ...,\n",
      "        [56, 43, 61,  ..., 52, 53, 52],\n",
      "        [50, 47, 43,  ..., 43, 39, 58],\n",
      "        [ 8,  0,  0,  ..., 25, 63,  1]])\n",
      "Output shape torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "#x, y = get_batch(training_data)\n",
    "#print(f\"\"\"Input {x}\n",
    "#Input shape {x.shape}\n",
    "#Output {y}\n",
    "#Output shape {y.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b4611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee6b4611",
    "outputId": "16f7cbcf-140b-44b6-9805-e4366f0b8585"
   },
   "outputs": [],
   "source": [
    "#for b in range(BATCH_SIZE):\n",
    "#    for t in range(BLOCK_SIZE):\n",
    "#        context = x[b,:t+1]\n",
    "#        target = y[b,t]\n",
    "#        \n",
    "#        print(f\"Input {context} -> Target {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb48bf",
   "metadata": {},
   "source": [
    "## Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581cb72",
   "metadata": {
    "id": "2581cb72"
   },
   "outputs": [],
   "source": [
    "class head(nn.Module):\n",
    "    \"\"\"\n",
    "    This class creates a head of a self-attention model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(NUM_EMBED, head_size, bias=False)\n",
    "        self.query = nn.Linear(NUM_EMBED, head_size, bias=False)\n",
    "        self.value = nn.Linear(NUM_EMBED, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(BLOCK_SIZE, \n",
    "                                                          BLOCK_SIZE)))\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)                    # Dimension: (B,T,C)\n",
    "        q = self.query(x)                  # Dimension: (B,T,C)\n",
    "        \n",
    "        # computating attention scores\n",
    "        # Attention(k,q,v) = softmax((q @ k.T) / sqrt(c)) @ v\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T,:T] == 0, \n",
    "                                      float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        outcome = weights @ v\n",
    "        \n",
    "        return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9580d",
   "metadata": {
    "id": "37e9580d"
   },
   "outputs": [],
   "source": [
    "class multiple_head_attention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class instance can create multiple attention head \n",
    "    instances and run them parallely.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([head(head_size) for _ in range(NUM_HEADS)])\n",
    "        self.projection = nn.Linear(NUM_EMBED, NUM_EMBED)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outcome = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        outcome = self.projection(outcome)\n",
    "        \n",
    "        return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e46b9f",
   "metadata": {
    "id": "48e46b9f"
   },
   "outputs": [],
   "source": [
    "class forward_propagation(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-layered Perceptron\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(NUM_EMBED, NUM_EMBED*4), \n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(NUM_EMBED*4, NUM_EMBED),\n",
    "                                 nn.Dropout(DROPOUT))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf1bf6",
   "metadata": {
    "id": "bcaf1bf6"
   },
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a transformer block where the communication i.e., \n",
    "    Attention scores are obtained and then passed into a simple\n",
    "    feedforward perceptron model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = NUM_EMBED // NUM_HEADS\n",
    "        self.self_attention_heads = multiple_head_attention(head_size)\n",
    "        self.feedforward = forward_propagation()\n",
    "        self.layernorm1 = nn.LayerNorm(NUM_EMBED)\n",
    "        self.layernorm2 = nn.LayerNorm(NUM_EMBED)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention_heads(self.layernorm1(x))\n",
    "        x = x + self.feedforward(self.layernorm2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a4fda",
   "metadata": {
    "id": "ff0a4fda"
   },
   "outputs": [],
   "source": [
    "class bigram_language_model(nn.Module):\n",
    "    \"\"\"\n",
    "    This model is a bigram language model. It takes a set of tokens as input.\n",
    "    Creates an embedding space. Generates new tokens based on given token.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, NUM_EMBED)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, NUM_EMBED)\n",
    "        self.attention_blocks = nn.Sequential(*[block() for _ in range(NUM_LAYER)])\n",
    "        self.layernorm = nn.LayerNorm(NUM_EMBED)\n",
    "        self.linear_head = nn.Linear(NUM_EMBED, VOCAB_SIZE)\n",
    "\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        \n",
    "        token_embeddings = self.token_embedding_table(idx)                    # Dimesion : (B,T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T))  # Dimesion : (T,C) \n",
    "        x = token_embeddings + position_embeddings \n",
    "        x = self.attention_blocks(x)\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.linear_head(x)                                          \n",
    "        \n",
    "        if targets is None: loss = None\n",
    "        else:\n",
    "            \n",
    "            # B: batch_size, T: sequence length, C: embedding vector dimension\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is the (B,T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # slice the idx upto last block_size length\n",
    "            idx_conditioned = idx[:, -BLOCK_SIZE:]\n",
    "            \n",
    "            # getting the predictions\n",
    "            logits, loss = self(idx_conditioned)\n",
    "            \n",
    "            # focus on the last time step \n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            # apply softmax to get the probabilities\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df934e00",
   "metadata": {},
   "source": [
    "### Evaluation method\n",
    "#### During training, after certain interval it evaluates the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c7243",
   "metadata": {
    "id": "405c7243"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    outcome = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\",\"validation\"]:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            if split == \"train\":\n",
    "                x,y = get_batch(training_data)\n",
    "            else: \n",
    "                x,y = get_batch(validation_data)\n",
    "            logits, loss = model(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        outcome[split] = losses.mean()\n",
    "    model.train()\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e5fd5",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "c22e5fd5",
    "outputId": "6400bd69-e0d4-4453-e1c4-8265adada5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 -- train_loss 4.3355 -- validation_loss 4.3434\n",
      "Step 501 -- train_loss 1.8924 -- validation_loss 1.9948\n",
      "Step 1001 -- train_loss 1.6461 -- validation_loss 1.8140\n",
      "Step 1501 -- train_loss 1.5453 -- validation_loss 1.7098\n",
      "Step 2001 -- train_loss 1.4838 -- validation_loss 1.6692\n",
      "Step 2501 -- train_loss 1.4480 -- validation_loss 1.6407\n",
      "Step 3001 -- train_loss 1.4233 -- validation_loss 1.6208\n",
      "Step 3501 -- train_loss 1.3999 -- validation_loss 1.5969\n",
      "Step 4001 -- train_loss 1.3879 -- validation_loss 1.5951\n",
      "Step 4501 -- train_loss 1.3665 -- validation_loss 1.5731\n",
      "Step 5001 -- train_loss 1.3557 -- validation_loss 1.5682\n",
      "Step 5501 -- train_loss 1.3469 -- validation_loss 1.5573\n",
      "Step 6001 -- train_loss 1.3370 -- validation_loss 1.5530\n",
      "Step 6501 -- train_loss 1.3310 -- validation_loss 1.5460\n",
      "Step 7001 -- train_loss 1.3181 -- validation_loss 1.5399\n",
      "Step 7501 -- train_loss 1.3147 -- validation_loss 1.5383\n",
      "Step 8001 -- train_loss 1.3065 -- validation_loss 1.5426\n",
      "Step 8501 -- train_loss 1.3046 -- validation_loss 1.5360\n",
      "Step 9001 -- train_loss 1.2959 -- validation_loss 1.5298\n",
      "Step 9501 -- train_loss 1.2913 -- validation_loss 1.5246\n",
      "1.3648021221160889\n"
     ]
    }
   ],
   "source": [
    "model = bigram_language_model()\n",
    "logits, loss = model(x,y)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for steps in range(MAX_ITERATION):\n",
    "    \n",
    "    if steps % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {steps+1} -- train_loss {losses['train']:.4f} -- validation_loss {losses['validation']:.4f}\")\n",
    "    \n",
    "    # generates batch of training data\n",
    "    x, y = get_batch(training_data)\n",
    "    \n",
    "    # forward pass \n",
    "    #evaluate the loss\n",
    "    logits, loss = model(x,y)\n",
    "    \n",
    "    # resets gradients of optimizer\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # computes gradients for backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    #updates parameters by backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56b3a2",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2b56b3a2",
    "outputId": "9a9de863-2067-4e67-fe29-5fffd5cd7824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thundo-bellike full of corrupt bittering fantard.\n",
      "For noisome and spirit! and well! Ist thou did,\n",
      "What raised tombles myself.\n",
      "\n",
      "Lord Marsare Hastily lour, thour such a broats upon\n",
      "the shemorse hath wadom, as I give to my inform,\n",
      "Exceive Hereford, i' the tide beyong from you,\n",
      "For this combles too, that your carrions,\n",
      "And what the doils his never crown;\n",
      "And far die, yet pensuage, and oft the Captain Delight\n",
      "As it off here that and day the Tower,\n",
      "Which a Paulina to consul, or to Juliet?\n",
      "\n",
      "Sirs, Murderer:\n",
      "Dost you shall not so.\n",
      "\n",
      "POMPER?\n",
      "\n",
      "JULIET:\n",
      "No; he should me now? who: I smay, I know? will not such\n",
      "privator? what of their taken yess he is: yet\n",
      "His must this off this him I had prove\n",
      "a cheepinion shephy kinders puddink, let us no; think,\n",
      "His likelf me can prend the horse speeds egarled:\n",
      "How speaks' sak your young so, for I shall\n",
      "The lamentain!\n",
      "\n",
      "PARIS:\n",
      "My lord, for weigness is for thee to keep now,\n",
      "Mow he is days, with his ears.' let's fice\n",
      "His mistress, even vain. Isabel so, resolution.\n",
      "\n",
      "Pr\n"
     ]
    }
   ],
   "source": [
    "def generate_from_token(model):\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "    gen_tokens = model.generate(idx, \n",
    "                                max_new_tokens=MAX_NEW_TOKENS)[0].tolist()\n",
    "    return decode(gen_tokens)\n",
    "\n",
    "print(generate_from_token(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918c634",
   "metadata": {
    "id": "3918c634"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
